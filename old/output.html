
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Markdown to HTML</title>
</head>
<body>
<h1><p align="center">Chapter 1: Introduction</p></h1>
<p>It is with great pleasure and anticipation that I announce the inauguration of our forthcoming series of Blogs explaining in great details about Optimisation techniques used in Machine Learning.</p>
<p>We will follow the book - <strong>The Elements of Differentiable Programming</strong> by <em>Blondel, Mathieu</em> and <em>Roulet, Vincent</em></p>
<blockquote>
<p>Automatic differentiation is like having your computer tell you the gradient of a function without resorting to finite difference approximation or coding an analytic derivative by hand. Itâ€™s a powerful tool used in machine learning, optimization, neural networks, and more.</p>
</blockquote>
<p>That quote might feel very overwhelming to you, but it is just describing what <em>PyTorch</em>, <em>TensorFlow</em> and <em>Jax</em> are. They are simply tools to do AutoDiff for us and make our life simpler.</p>
<p>We will follow the books flow in covering all the topics:</p>
<ol>
<li>Fundamentals: Differentiation and Probabilistic Learning</li>
<li>Differentiable Programs: Neural networks, Sequence Networks and Control Flows</li>
<li>Differentiating through Programs:</li>
<li>Smoothing Programs:</li>
</ol>
<p>At the end of each Article i will give some simple task or problem you can send the solution to my Email: admin@greatrsingh.in</p>
<p>Well we have reached the end of it so here is the task:</p>
<ol>
<li>Explain <strong>Gradient Descent</strong> Algorithm and investigate its history.</li>
</ol>
<hr />
<p>References and Useful Resources:</p>
<ol>
<li><a href="https://arxiv.org/pdf/2403.14606.pdf">The Elements of Differentiable Programming</a> by <em>Blondel, Mathieu</em> and <em>Roulet, Vincent</em>.</li>
</ol>
</body>
</html>
